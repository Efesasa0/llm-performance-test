{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from langchain_community.llms.ollama import Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasticity of the Model tests\n",
    "\n",
    "Here can be used to view how each model outputs a response. Can be used to do a personal preference assessment of each output as a first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompts = [\"is the sky blue\", \"Hello\", \"I need help with staying healthy\"]\n",
    "model_names = [f\"hal{i}000:latest\" for i in range(0,11)]\n",
    "for model in model_names:\n",
    "    llm=Ollama(model=model)\n",
    "    out1=llm.invoke(example_prompts[2])\n",
    "    #print(f\"{model} answered: {out1}\\n{\"*\"*20}\\n{\"#\"*20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "We load the data which contains responses for prompts in *prompts.json* for all selected models with tuned system prompts. For each prompt also includes time taken to respond.\n",
    "\n",
    "Following models are used:\n",
    "1. phi2:2.7\n",
    "2. phi2:chat\n",
    "3. llama2:7b\n",
    "4. llama2:7b:chat\n",
    "5. llava2:7b\n",
    "6. llava2:13b\n",
    "7. mistral:7b:instruct\n",
    "8. mistral:7b:latest\n",
    "9. neural-chat:7b\n",
    "10. tinyllama:1.1b:chat\n",
    "11. gemma:2b:instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ollama_out.json\", 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.221836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.151013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.220663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.276666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.601502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.180190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.123408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prompt_times\n",
       "count     45.000000\n",
       "mean       1.221836\n",
       "std        1.151013\n",
       "min        0.220663\n",
       "25%        0.276666\n",
       "50%        0.601502\n",
       "75%        2.180190\n",
       "max        4.123408"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(json_data)\n",
    "df[df[\"model_names\"]==\"hal10000:latest\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD PART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ollama_out.json\", 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"base_names = [\"phi2:2.7\", \"phi2:chat\", \"llama2:7b\", \"llama2:7b:chat\", \"llava2:7b\", \"llava2:13b\", \"mistral:7b:instruct\", \"mistral:7b:latest\", \"neural-chat:7b\", \"tinyllama:1.1b:chat\", \"gemma:2b:instruct\"]\n",
    "new_names = []\n",
    "for key in json_data.keys():\n",
    "    new_names.append(f\"{key}\")\n",
    "    \n",
    "data = {\"mean\":[None for i in range(11)],\n",
    "    \"max\":[None for i in range(11)],\n",
    "    \"min\":[None for i in range(11)],\n",
    "    \"std\":[None for i in range(11)],\n",
    "    \"median\":[None for i in range(11)],\n",
    "    \"variance\":[None for i in range(11)]}\n",
    "\n",
    "df_speed = pd.DataFrame(data=data, index=new_names)\n",
    "prompts = [f\"prompt_{i}\" for i in range(9)]\n",
    "for model_name in json_data.keys():\n",
    "    mean_time= []\n",
    "    max_time = []\n",
    "    min_time = []\n",
    "    std_time = []\n",
    "    median_time = []\n",
    "    variance_time = []\n",
    "    for prompt in prompts:\n",
    "        stats = json_data[model_name][model_name][prompt]['time_ls']\n",
    "        mean_time.append(np.mean(stats))\n",
    "        max_time.append(np.max(stats))\n",
    "        min_time.append(np.min(stats))\n",
    "        std_time.append(np.std(stats))\n",
    "        median_time.append(np.median(stats))\n",
    "        variance_time.append(np.var(stats))\n",
    "    df_speed.loc[model_name, \"mean\"] = np.mean(mean_time)\n",
    "    df_speed.loc[model_name, \"max\"] = np.mean(max_time)\n",
    "    df_speed.loc[model_name, \"min\"] = np.mean(min_time)\n",
    "    df_speed.loc[model_name, \"std\"] = np.mean(std_time)\n",
    "    df_speed.loc[model_name, \"median\"] = np.mean(median_time)\n",
    "    df_speed.loc[model_name, \"variance\"] = np.mean(variance_time)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean        max       min       std     median  variance\n",
      "hal0000:latest   11.049726  12.158017  9.941436   1.10829  11.049726  2.435282\n",
      "hal1000:latest    9.263446  11.971134  6.555757  2.707689   9.263446  18.47316\n",
      "hal2000:latest    6.488203   8.162406  4.814001  1.674203   6.488203  7.515539\n",
      "hal3000:latest     8.30513   9.499569  7.110691  1.194439    8.30513    4.5875\n",
      "hal4000:latest    5.339737   6.736031  3.943443  1.396294   5.339737  4.110405\n",
      "hal5000:latest    7.100469   8.180498   6.02044  1.080029   7.100469  1.939389\n",
      "hal6000:latest    7.974267   9.656914  6.291619  1.682647   7.974267  4.861753\n",
      "hal7000:latest    7.294879   8.359022  6.230735  1.064143   7.294879  1.342113\n",
      "hal8000:latest     7.80681   8.871983  6.741638  1.065172    7.80681  1.705915\n",
      "hal9000:latest    1.812049   2.188055  1.436044  0.376005   1.812049  0.222088\n",
      "hal10000:latest   2.726248   2.935577   2.51692  0.209328   2.726248  0.111513\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print(df_speed)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
